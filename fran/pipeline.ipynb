{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4c5d3bd",
   "metadata": {},
   "source": [
    "# PIPELINE v0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ccdcd2",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f29cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bulacio/Library/CloudStorage/OneDrive-UNIVERSIDADDEMURCIA/0. DTU/02456-deeplearning/project/Deep-Learning-Transformers/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import torch\n",
    "import os\n",
    "from typing import List, Type\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "# Database Imports\n",
    "from sqlmodel import SQLModel, Field, Session, create_engine, select\n",
    "from sqlalchemy import Column, text\n",
    "from pgvector.sqlalchemy import Vector\n",
    "\n",
    "# ML Imports\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf577a8",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f7a52b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database Connection\n",
    "DATABASE_URL = \"postgresql+psycopg://bulacio@localhost:5432/vectordb\"\n",
    "\n",
    "# Dataset Path\n",
    "DATASET_PATH = \"../data_filtered/corpus_filtered.jsonl\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ac9c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment A: bert all-MiniLM-L6-v2\n",
    "CURRENT_MODEL_ID = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "CURRENT_TABLE_NAME = 'bert_all_minilm_l6_v2'\n",
    "VECTOR_DIMENSION = 384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd3b3e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment B: \n",
    "# CURRENT_MODEL_ID = \n",
    "# CURRENT_TABLE_NAME = \n",
    "# VECTOR_DIMENSION = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365149ec",
   "metadata": {},
   "source": [
    "## BERT Embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6140fca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertEmbedder:\n",
    "    \"\"\"\n",
    "    Encapsulates the Transformer model logic.\n",
    "    Initialized with a specific Model ID to allow easy swapping.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_id: str):\n",
    "        print(f\"--- Loading Model: {model_id} ---\")\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "        self.model = AutoModel.from_pretrained(model_id).to(self.device)\n",
    "\n",
    "    def _mean_pooling(self, model_output, attention_mask):\n",
    "        \"\"\"\n",
    "        Internal method: Collapses the token matrix into a single vector.\n",
    "        \"\"\"\n",
    "        token_embeddings = model_output.last_hidden_state\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        \n",
    "        sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "        sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "        \n",
    "        return sum_embeddings / sum_mask\n",
    "\n",
    "    def generate_embedding(self, text: str) -> List[float]:\n",
    "        # A. Tokenize\n",
    "        inputs = self.tokenizer(text, padding=True, truncation=True, return_tensors='pt').to(self.device)\n",
    "\n",
    "        # B. Inference\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "\n",
    "        # C. Pooling\n",
    "        sentence_embeddings = self._mean_pooling(outputs, inputs['attention_mask'])\n",
    "\n",
    "        # D. Normalize\n",
    "        sentence_embeddings = torch.nn.functional.normalize(sentence_embeddings, p=2, dim=1)\n",
    "\n",
    "        # Return list (move to CPU first if on GPU)\n",
    "        return sentence_embeddings[0].cpu().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2adb7f",
   "metadata": {},
   "source": [
    "## dynamic database creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "102d2c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table_class(table_name: str, dim: int) -> Type[SQLModel]:\n",
    "    \"\"\"\n",
    "    Dynamically creates a SQLModel class.\n",
    "    This allows us to save data to different tables (e.g., 'bert_v1', 'bert_v2')\n",
    "    without rewriting the class code manually.\n",
    "    \"\"\"\n",
    "    # We define the class attributes dynamically\n",
    "    class DynamicTable(SQLModel, table=True):\n",
    "        __tablename__ = table_name\n",
    "        __table_args__ = {'extend_existing': True} # Allows overwriting if class exists in memory\n",
    "\n",
    "        # Mapping CSV '_id' to primary key\n",
    "        id: str = Field(primary_key=True) \n",
    "        title: str\n",
    "        text: str\n",
    "        \n",
    "        # The Vector column\n",
    "        embedding: List[float] = Field(sa_column=Column(Vector(dim)))\n",
    "\n",
    "    return DynamicTable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afb985d",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e44e61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline():\n",
    "    # A. Setup Database\n",
    "    engine = create_engine(DATABASE_URL)\n",
    "    \n",
    "    # Ensure pgvector extension exists\n",
    "    with engine.connect() as conn:\n",
    "        conn.execute(text(\"CREATE EXTENSION IF NOT EXISTS vector\"))\n",
    "        conn.commit()\n",
    "\n",
    "    # B. Define the Table Model based on configuration\n",
    "    TableClass = create_table_class(CURRENT_TABLE_NAME, VECTOR_DIMENSION)\n",
    "    SQLModel.metadata.create_all(engine)\n",
    "\n",
    "    # C. Initialize ML Model\n",
    "    embedder = BertEmbedder(CURRENT_MODEL_ID)\n",
    "\n",
    "    # D. Process JSONL and Insert\n",
    "    if not os.path.exists(DATASET_PATH):\n",
    "        print(f\"Error: Dataset not found at {DATASET_PATH}\")\n",
    "        return\n",
    "\n",
    "    print(f\"--- Processing JSONL: {DATASET_PATH} ---\")\n",
    "    print(f\"--- Target Table: {CURRENT_TABLE_NAME} ---\")\n",
    "\n",
    "    data_buffer = []\n",
    "    BATCH_SIZE = 100 \n",
    "\n",
    "    with Session(engine) as session:\n",
    "        # Open the JSONL file\n",
    "        with open(DATASET_PATH, mode='r', encoding='utf-8') as f:\n",
    "            \n",
    "            # Iterate line by line. \n",
    "            # We wrap 'f' with tqdm to show progress (lines processed)\n",
    "            for line in tqdm(f, desc=\"Embedding Docs\"):\n",
    "                try:\n",
    "                    if not line.strip():\n",
    "                        continue # Skip empty lines\n",
    "\n",
    "                    # 1. Parse JSON\n",
    "                    row = json.loads(line)\n",
    "\n",
    "                    # 2. Extract Data\n",
    "                    # Assuming keys are: \"_id\", \"title\", \"text\" based on your previous CSV structure\n",
    "                    doc_id = row.get('_id')\n",
    "                    title = row.get('title', '')\n",
    "                    doc_text = row.get('text', '')\n",
    "\n",
    "                    # Skip if ID is missing\n",
    "                    if not doc_id:\n",
    "                        continue\n",
    "\n",
    "                    # 3. Generate Vector\n",
    "                    # Combine title and text for better semantic search context\n",
    "                    full_content = f\"{title}: {doc_text}\"\n",
    "                    vector = embedder.generate_embedding(full_content)\n",
    "\n",
    "                    # 4. Create Record\n",
    "                    record = TableClass(\n",
    "                        id=doc_id,\n",
    "                        title=title,\n",
    "                        text=doc_text,\n",
    "                        embedding=vector\n",
    "                    )\n",
    "                    data_buffer.append(record)\n",
    "\n",
    "                    # 5. Batch Commit\n",
    "                    if len(data_buffer) >= BATCH_SIZE:\n",
    "                        session.add_all(data_buffer)\n",
    "                        session.commit()\n",
    "                        data_buffer = []\n",
    "\n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"Skipping invalid JSON line\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing doc {doc_id if 'doc_id' in locals() else 'unknown'}: {e}\")\n",
    "\n",
    "            # 6. Commit remaining records after loop finishes\n",
    "            if data_buffer:\n",
    "                session.add_all(data_buffer)\n",
    "                session.commit()\n",
    "\n",
    "    print(\"\\n--- Pipeline Finished Successfully ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23555964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading Model: sentence-transformers/all-MiniLM-L6-v2 ---\n",
      "Using device: cpu\n",
      "--- Processing JSONL: ../data_filtered/corpus_filtered.jsonl ---\n",
      "--- Target Table: embeddings_minilm ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding Docs: 379it [00:14, 27.05it/s]"
     ]
    }
   ],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "# Run the full process\n",
    "run_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c35533",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning-transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
