{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5265ab0f",
   "metadata": {},
   "source": [
    "# BERT EMBEDDINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5830682e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/dtu02456/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01361ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer and model: sentence-transformers/all-MiniLM-L6-v2...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1. Configuration\n",
    "# ----------------\n",
    "# We use the MiniLM model (BERT distilled).\n",
    "# Output dimension: 384\n",
    "MODEL_ID = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "\n",
    "print(f\"Loading tokenizer and model: {MODEL_ID}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "model = AutoModel.from_pretrained(MODEL_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4597882c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Mathematical Logic (Mean Pooling)\n",
    "# ----------------\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    \"\"\"\n",
    "    Collapses the matrix of token vectors into a single sentence vector\n",
    "    by calculating the weighted average, ignoring padding tokens.\n",
    "    \"\"\"\n",
    "    # The first element of model_output contains all token embeddings\n",
    "    token_embeddings = model_output.last_hidden_state \n",
    "    \n",
    "    # Expand attention_mask to match the size of embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    \n",
    "    # Sum the embeddings of valid tokens (mask value = 1)\n",
    "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "    \n",
    "    # Count valid tokens (clamping min to 1e-9 to avoid division by zero)\n",
    "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    \n",
    "    # Calculate Mean\n",
    "    return sum_embeddings / sum_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a92ab89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Main Inference Function\n",
    "# ----------------\n",
    "def get_embedding(text: str):\n",
    "    # A. Tokenize\n",
    "    # Convert text to BERT input format (Add [CLS], [SEP], padding)\n",
    "    encoded_input = tokenizer(text, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "    # B. Model Inference\n",
    "    with torch.no_grad(): # Disable gradient calculation to save memory\n",
    "        model_output = model(**encoded_input)\n",
    "\n",
    "    # C. Pooling\n",
    "    # Convert token vectors -> Single vector\n",
    "    sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "    # D. Normalization\n",
    "    # Normalize result (Length = 1) for Cosine Similarity usage\n",
    "    sentence_embeddings = torch.nn.functional.normalize(sentence_embeddings, p=2, dim=1)\n",
    "\n",
    "    # Return as a standard Python list (or numpy array)\n",
    "    return sentence_embeddings[0].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac2c5d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Input Text: 'This is a test sentence for BERT embedding.'\n",
      "Vector Dimension: 384\n",
      "First 5 values: [-0.03410151228308678, -0.003907477017492056, 0.08655714243650436, 0.036635272204875946, 0.006490279454737902]\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "#if __name__ == \"__main__\":\n",
    "text = \"cat\"\n",
    "\n",
    "vector = get_embedding(text)\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"Input Text: '{text}'\")\n",
    "print(f\"Vector Dimension: {len(vector)}\") # Should be 384\n",
    "print(f\"First 5 values: {vector[:5]}\")\n",
    "print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd9f6df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dtu02456",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
