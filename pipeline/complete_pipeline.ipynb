{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16372c7f",
   "metadata": {},
   "source": [
    "# Complete Deep Learning Pipeline\n",
    "Complete Pipeline: Preprocess ‚Üí Tokenizer ‚Üí Train Models ‚Üí Embeddings ‚Üí Evaluation\n",
    "\n",
    "This notebook orchestrates the entire pipeline with configurable parameters at the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c072c581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PIPELINE CONFIGURATION\n",
      "================================================================================\n",
      "\n",
      "üìç SKIP FLAGS:\n",
      "  SKIP_PREPROCESS: False\n",
      "  SKIP_TOKENIZER: True\n",
      "  SKIP_LSTM: False\n",
      "  SKIP_TRANSFORMER: False\n",
      "  SKIP_EMBEDDINGS: False\n",
      "  SKIP_EVALUATION: False\n",
      "\n",
      "‚öôÔ∏è PARAMETERS:\n",
      "  Corpus size: 10\n",
      "  Tokenizer vocab size: 2000\n",
      "  LSTM epochs: 1, batch size: 32, seq length: 128\n",
      "  Transformer epochs: 1, batch size: 32, seq length: 128\n",
      "  Embeddings models: None\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# üîß CONFIGURATION - MODIFY THESE BEFORE RUNNING\n",
    "# ============================================================================\n",
    "\n",
    "# ========== SKIP FLAGS - Set to True to skip a stage ==========\n",
    "SKIP_PREPROCESS = False\n",
    "SKIP_TOKENIZER = True\n",
    "SKIP_LSTM = False\n",
    "SKIP_TRANSFORMER = False\n",
    "SKIP_EMBEDDINGS = False\n",
    "SKIP_EVALUATION = False\n",
    "\n",
    "# ========== PREPROCESSING PARAMETERS ==========\n",
    "CORPUS_SIZE = 10      # Tiny data size for testing\n",
    "\n",
    "# ========== TOKENIZER PARAMETERS ==========\n",
    "TOKENIZER_VOCAB_SIZE = 2000\n",
    "\n",
    "# ========== LSTM TRAINING PARAMETERS ==========\n",
    "LSTM_EPOCHS = 1         # Just for testing small number\n",
    "LSTM_BATCH_SIZE = 32\n",
    "LSTM_SEQ_LENGTH = 128\n",
    "LSTM_LEARNING_RATE = 0.001\n",
    "\n",
    "# ========== TRANSFORMER TRAINING PARAMETERS ==========\n",
    "TRANSFORMER_EPOCHS = 1 # Same here\n",
    "TRANSFORMER_BATCH_SIZE = 32\n",
    "TRANSFORMER_SEQ_LENGTH = 128\n",
    "TRANSFORMER_LEARNING_RATE = 0.001\n",
    "\n",
    "# ========== EMBEDDINGS PARAMETERS ==========\n",
    "EMBEDDINGS_MODELS = None  # None = all models ['byt5', 'canine', 'bpe-lstm', 'bpe-transformer', 'bert']\n",
    "EMBEDDINGS_CLEAR_EXISTING = True\n",
    "\n",
    "# ============================================================================\n",
    "# Display current configuration\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"PIPELINE CONFIGURATION\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nüìç SKIP FLAGS:\")\n",
    "print(f\"  SKIP_PREPROCESS: {SKIP_PREPROCESS}\")\n",
    "print(f\"  SKIP_TOKENIZER: {SKIP_TOKENIZER}\")\n",
    "print(f\"  SKIP_LSTM: {SKIP_LSTM}\")\n",
    "print(f\"  SKIP_TRANSFORMER: {SKIP_TRANSFORMER}\")\n",
    "print(f\"  SKIP_EMBEDDINGS: {SKIP_EMBEDDINGS}\")\n",
    "print(f\"  SKIP_EVALUATION: {SKIP_EVALUATION}\")\n",
    "print(\"\\n‚öôÔ∏è PARAMETERS:\")\n",
    "print(f\"  Corpus size: {CORPUS_SIZE}\")\n",
    "print(f\"  Tokenizer vocab size: {TOKENIZER_VOCAB_SIZE}\")\n",
    "print(f\"  LSTM epochs: {LSTM_EPOCHS}, batch size: {LSTM_BATCH_SIZE}, seq length: {LSTM_SEQ_LENGTH}\")\n",
    "print(f\"  Transformer epochs: {TRANSFORMER_EPOCHS}, batch size: {TRANSFORMER_BATCH_SIZE}, seq length: {TRANSFORMER_SEQ_LENGTH}\")\n",
    "print(f\"  Embeddings models: {EMBEDDINGS_MODELS}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09f48c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook directory: c:\\Users\\nick\\Desktop\\DTU Courses\\02456 Deep Learning\\Deep-Learning-Transformers\n",
      "Repository root: c:\\Users\\nick\\Desktop\\DTU Courses\\02456 Deep Learning\\Deep-Learning-Transformers\n",
      "Current working directory: c:\\Users\\nick\\Desktop\\DTU Courses\\02456 Deep Learning\\Deep-Learning-Transformers\n",
      "Python path updated\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SETUP: Import Libraries & Set Path\n",
    "# ============================================================================\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "# Add repo root to path - go up from pipeline dir to repo root\n",
    "notebook_dir = Path.cwd()\n",
    "repo_root = notebook_dir.parent if notebook_dir.name == 'pipeline' else notebook_dir\n",
    "\n",
    "# Add repo root to Python path\n",
    "if str(repo_root) not in sys.path:\n",
    "    sys.path.insert(0, str(repo_root))\n",
    "\n",
    "# Change working directory to repo root\n",
    "os.chdir(repo_root)\n",
    "\n",
    "print(f\"Notebook directory: {notebook_dir}\")\n",
    "print(f\"Repository root: {repo_root}\")\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "print(f\"Python path updated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bd0818",
   "metadata": {},
   "source": [
    "## Stage 1: Preprocessing\n",
    "Preprocess NQ dataset: filter corpus and align queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74f2bbc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STAGE 1: PREPROCESSING\n",
      "================================================================================\n",
      "\n",
      "Parameters:\n",
      "  Corpus size: 10\n",
      "Loading corpus dataset...\n",
      "  Total documents: 2681468\n",
      "  Total documents: 2681468\n",
      "  Unique titles: 108593\n",
      "\n",
      "Filtering corpus to 10 documents...\n",
      "  Unique titles: 108593\n",
      "\n",
      "Filtering corpus to 10 documents...\n",
      "  Filtered documents: 116\n",
      "  Unique titles: 10\n",
      "  Saved to: c:\\Users\\nick\\Desktop\\DTU Courses\\02456 Deep Learning\\Deep-Learning-Transformers\\data_processing\\..\\data_filtered\\corpus_filtered.jsonl\n",
      "\n",
      "Loading queries dataset...\n",
      "  Total queries: 3452\n",
      "Loading relevance judgments...\n",
      "  Total query-corpus pairs: 4201\n",
      "\n",
      "Merging queries with relevance judgments...\n",
      "  Filtered documents: 116\n",
      "  Unique titles: 10\n",
      "  Saved to: c:\\Users\\nick\\Desktop\\DTU Courses\\02456 Deep Learning\\Deep-Learning-Transformers\\data_processing\\..\\data_filtered\\corpus_filtered.jsonl\n",
      "\n",
      "Loading queries dataset...\n",
      "  Total queries: 3452\n",
      "Loading relevance judgments...\n",
      "  Total query-corpus pairs: 4201\n",
      "\n",
      "Merging queries with relevance judgments...\n",
      "Filtering queries to match filtered corpus...\n",
      "  Filtered queries: 10\n",
      "  Saved to: c:\\Users\\nick\\Desktop\\DTU Courses\\02456 Deep Learning\\Deep-Learning-Transformers\\data_processing\\..\\data_filtered\\queries_filtered.jsonl\n",
      "\n",
      "[OK] Preprocessing complete\n",
      "  Corpus: c:\\Users\\nick\\Desktop\\DTU Courses\\02456 Deep Learning\\Deep-Learning-Transformers\\data_processing\\..\\data_filtered\\corpus_filtered.jsonl\n",
      "  Queries: c:\\Users\\nick\\Desktop\\DTU Courses\\02456 Deep Learning\\Deep-Learning-Transformers\\data_processing\\..\\data_filtered\\queries_filtered.jsonl\n",
      "Filtering queries to match filtered corpus...\n",
      "  Filtered queries: 10\n",
      "  Saved to: c:\\Users\\nick\\Desktop\\DTU Courses\\02456 Deep Learning\\Deep-Learning-Transformers\\data_processing\\..\\data_filtered\\queries_filtered.jsonl\n",
      "\n",
      "[OK] Preprocessing complete\n",
      "  Corpus: c:\\Users\\nick\\Desktop\\DTU Courses\\02456 Deep Learning\\Deep-Learning-Transformers\\data_processing\\..\\data_filtered\\corpus_filtered.jsonl\n",
      "  Queries: c:\\Users\\nick\\Desktop\\DTU Courses\\02456 Deep Learning\\Deep-Learning-Transformers\\data_processing\\..\\data_filtered\\queries_filtered.jsonl\n"
     ]
    }
   ],
   "source": [
    "def stage_preprocess():\n",
    "    \"\"\"Preprocess NQ dataset: filter corpus and align queries\"\"\"\n",
    "    if SKIP_PREPROCESS:\n",
    "        print(\"\\n[SKIP] Preprocessing\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STAGE 1: PREPROCESSING\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    from data_processing.nq_preprocess import preprocess_data\n",
    "    \n",
    "    try:\n",
    "        print(f\"\\nParameters:\")\n",
    "        print(f\"  Corpus size: {CORPUS_SIZE}\")\n",
    "        \n",
    "        corpus_file, queries_file = preprocess_data(corpus_size=CORPUS_SIZE)\n",
    "        print(f\"\\n[OK] Preprocessing complete\")\n",
    "        print(f\"  Corpus: {corpus_file}\")\n",
    "        print(f\"  Queries: {queries_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n[ERROR] Preprocessing failed: {e}\")\n",
    "        raise\n",
    "\n",
    "# Run preprocessing stage\n",
    "stage_preprocess()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b997eee",
   "metadata": {},
   "source": [
    "## Stage 2: Tokenizer Training\n",
    "Train BPE tokenizer on dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0dcb89c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[SKIP] Tokenizer training\n"
     ]
    }
   ],
   "source": [
    "def stage_tokenizer():\n",
    "    \"\"\"Train BPE tokenizer on dataset\"\"\"\n",
    "    if SKIP_TOKENIZER:\n",
    "        print(\"\\n[SKIP] Tokenizer training\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STAGE 2: TOKENIZER TRAINING\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    tokenizer_script = repo_root / 'tokenization' / 'our_tokenizers' / 'train_tokenizer.py'\n",
    "    \n",
    "    try:\n",
    "        print(f\"\\nParameters:\")\n",
    "        print(f\"  Vocab size: {TOKENIZER_VOCAB_SIZE}\")\n",
    "        print(f\"\\nRunning tokenizer training...\")\n",
    "        result = subprocess.run(\n",
    "            [sys.executable, str(tokenizer_script)],\n",
    "            cwd=repo_root / 'tokenization' / 'our_tokenizers',\n",
    "            check=True,\n",
    "            capture_output=False\n",
    "        )\n",
    "        print(f\"\\n[OK] Tokenizer training complete\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"\\n[ERROR] Tokenizer training failed with exit code {e.returncode}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"\\n[ERROR] Tokenizer training failed: {e}\")\n",
    "        raise\n",
    "\n",
    "# Run tokenizer stage\n",
    "stage_tokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f85cec",
   "metadata": {},
   "source": [
    "## Stage 3A: Train LSTM Model\n",
    "Train LSTM language model with BPE tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd7d7fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STAGE 3A: LSTM MODEL TRAINING\n",
      "================================================================================\n",
      "\n",
      "Parameters:\n",
      "  Epochs: 1\n",
      "  Batch size: 32\n",
      "  Sequence length: 128\n",
      "  Learning rate: 0.001\n",
      "============================================================\n",
      "Training LSTM Language Model with BPE Tokenization\n",
      "============================================================\n",
      "\n",
      "üîß Using device: cuda\n",
      "\n",
      "üì¶ Loading BPE tokenizer from c:\\Users\\nick\\Desktop\\DTU Courses\\02456 Deep Learning\\Deep-Learning-Transformers\\tokenization\\vocabularies\\bpe_tokenizer.json\n",
      "Tokenizer loaded from c:\\Users\\nick\\Desktop\\DTU Courses\\02456 Deep Learning\\Deep-Learning-Transformers\\tokenization\\vocabularies\\bpe_tokenizer.json\n",
      "   Vocabulary size: 2001\n",
      "\n",
      "üìö Loading documents from c:\\Users\\nick\\Desktop\\DTU Courses\\02456 Deep Learning\\Deep-Learning-Transformers\\data_filtered\\corpus_filtered.jsonl\n",
      "   Loaded 116 documents\n",
      "\n",
      "üî® Creating dataset...\n",
      "Creating dataset with seq_length=128, stride=64...\n",
      "  Processing text 0/116\n",
      "  Processing text 100/116\n",
      "‚úÖ Created 328 training examples\n",
      "\n",
      "üîß Using device: cuda\n",
      "\n",
      "üì¶ Loading BPE tokenizer from c:\\Users\\nick\\Desktop\\DTU Courses\\02456 Deep Learning\\Deep-Learning-Transformers\\tokenization\\vocabularies\\bpe_tokenizer.json\n",
      "Tokenizer loaded from c:\\Users\\nick\\Desktop\\DTU Courses\\02456 Deep Learning\\Deep-Learning-Transformers\\tokenization\\vocabularies\\bpe_tokenizer.json\n",
      "   Vocabulary size: 2001\n",
      "\n",
      "üìö Loading documents from c:\\Users\\nick\\Desktop\\DTU Courses\\02456 Deep Learning\\Deep-Learning-Transformers\\data_filtered\\corpus_filtered.jsonl\n",
      "   Loaded 116 documents\n",
      "\n",
      "üî® Creating dataset...\n",
      "Creating dataset with seq_length=128, stride=64...\n",
      "  Processing text 0/116\n",
      "  Processing text 100/116\n",
      "‚úÖ Created 328 training examples\n",
      "   Train: 262 examples\n",
      "   Val:   32 examples\n",
      "   Test:  34 examples\n",
      "\n",
      "üß† Creating LSTM model...\n",
      "   Train: 262 examples\n",
      "   Val:   32 examples\n",
      "   Test:  34 examples\n",
      "\n",
      "üß† Creating LSTM model...\n",
      "   Parameters: 2,079,185\n",
      "\n",
      "üöÄ Starting training for 1 epochs...\n",
      "   Parameters: 2,079,185\n",
      "\n",
      "üöÄ Starting training for 1 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:00<00:00, 10.69it/s, loss=6.4859, ppl=655.80] \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/1 (0.9s)\n",
      "  Train Loss: 7.4920 | Train PPL: 1793.68\n",
      "  Val Loss:   6.0539 | Val PPL:   425.79\n",
      "  LR: 0.001000\n",
      "  ‚úÖ Saved best model (val_loss=6.0539)\n",
      "\n",
      "üìä Final evaluation on test set...\n",
      "\n",
      "============================================================\n",
      "FINAL RESULTS\n",
      "============================================================\n",
      "Test Loss:       6.0520\n",
      "Test Perplexity: 424.98\n",
      "Bits per Char:   8.731\n",
      "============================================================\n",
      "\n",
      "‚úÖ Training complete! Model saved to c:\\Users\\nick\\Desktop\\DTU Courses\\02456 Deep Learning\\Deep-Learning-Transformers\\models\\lstm_bpe_final.pt\n",
      "\n",
      "[OK] LSTM training complete\n"
     ]
    }
   ],
   "source": [
    "def stage_train_lstm():\n",
    "    \"\"\"Train LSTM language model with BPE tokenization\"\"\"\n",
    "    if SKIP_LSTM:\n",
    "        print(\"\\n[SKIP] LSTM model training\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STAGE 3A: LSTM MODEL TRAINING\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    from models.LSTM.training.train_bpe_lstm import main as train_lstm_main\n",
    "    \n",
    "    try:\n",
    "        print(f\"\\nParameters:\")\n",
    "        print(f\"  Epochs: {LSTM_EPOCHS}\")\n",
    "        print(f\"  Batch size: {LSTM_BATCH_SIZE}\")\n",
    "        print(f\"  Sequence length: {LSTM_SEQ_LENGTH}\")\n",
    "        print(f\"  Learning rate: {LSTM_LEARNING_RATE}\")\n",
    "        \n",
    "        train_lstm_main(\n",
    "            batch_size=LSTM_BATCH_SIZE,\n",
    "            seq_length=LSTM_SEQ_LENGTH,\n",
    "            num_epochs=LSTM_EPOCHS,\n",
    "            learning_rate=LSTM_LEARNING_RATE\n",
    "        )\n",
    "        print(f\"\\n[OK] LSTM training complete\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n[ERROR] LSTM training failed: {e}\")\n",
    "        raise\n",
    "\n",
    "# Run LSTM training stage\n",
    "stage_train_lstm()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4707b594",
   "metadata": {},
   "source": [
    "## Stage 3B: Train Transformer Model\n",
    "Train Transformer language model with BPE tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ab5afc49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STAGE 3B: TRANSFORMER MODEL TRAINING\n",
      "================================================================================\n",
      "\n",
      "Parameters:\n",
      "  Epochs: 1\n",
      "  Batch size: 32\n",
      "  Sequence length: 128\n",
      "  Learning rate: 0.001\n",
      "============================================================\n",
      "Training Transformer Language Model with BPE Tokenization\n",
      "============================================================\n",
      "\n",
      "üîß Using device: cuda\n",
      "\n",
      "üì¶ Loading BPE tokenizer from c:\\Users\\nick\\Desktop\\DTU Courses\\02456 Deep Learning\\Deep-Learning-Transformers\\tokenization\\vocabularies\\bpe_tokenizer.json\n",
      "Tokenizer loaded from c:\\Users\\nick\\Desktop\\DTU Courses\\02456 Deep Learning\\Deep-Learning-Transformers\\tokenization\\vocabularies\\bpe_tokenizer.json\n",
      "   Vocabulary size: 2001\n",
      "\n",
      "üìö Loading documents from c:\\Users\\nick\\Desktop\\DTU Courses\\02456 Deep Learning\\Deep-Learning-Transformers\\data_filtered\\corpus_filtered.jsonl\n",
      "   Loaded 116 documents\n",
      "\n",
      "üî® Creating dataset...\n",
      "Creating dataset with seq_length=128, stride=64...\n",
      "  Processing text 0/116\n",
      "  Processing text 100/116\n",
      "‚úÖ Created 328 training examples\n",
      "   Train: 262 examples\n",
      "   Val:   32 examples\n",
      "   Test:  34 examples\n",
      "\n",
      "üß† Creating Transformer model...\n",
      "   Parameters: 2,607,825\n",
      "\n",
      "üöÄ Starting training for 1 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:00<00:00, 20.42it/s, loss=5.5424, ppl=255.30] \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/1 (0.5s)\n",
      "  Train Loss: 6.4789 | Train PPL: 651.28\n",
      "  Val Loss:   5.5096 | Val PPL:   247.05\n",
      "  LR: 0.001000\n",
      "  ‚úÖ Saved best model (val_loss=5.5096)\n",
      "\n",
      "üìä Final evaluation on test set...\n",
      "\n",
      "============================================================\n",
      "FINAL RESULTS\n",
      "============================================================\n",
      "Test Loss:       5.4612\n",
      "Test Perplexity: 235.39\n",
      "Bits per Char:   7.879\n",
      "============================================================\n",
      "\n",
      "‚úÖ Training complete! Model saved to c:\\Users\\nick\\Desktop\\DTU Courses\\02456 Deep Learning\\Deep-Learning-Transformers\\models\\Transformer\\transformer_bpe_final.pt\n",
      "\n",
      "[OK] Transformer training complete\n",
      "\n",
      "============================================================\n",
      "FINAL RESULTS\n",
      "============================================================\n",
      "Test Loss:       5.4612\n",
      "Test Perplexity: 235.39\n",
      "Bits per Char:   7.879\n",
      "============================================================\n",
      "\n",
      "‚úÖ Training complete! Model saved to c:\\Users\\nick\\Desktop\\DTU Courses\\02456 Deep Learning\\Deep-Learning-Transformers\\models\\Transformer\\transformer_bpe_final.pt\n",
      "\n",
      "[OK] Transformer training complete\n"
     ]
    }
   ],
   "source": [
    "def stage_train_transformer():\n",
    "    \"\"\"Train Transformer language model with BPE tokenization\"\"\"\n",
    "    if SKIP_TRANSFORMER:\n",
    "        print(\"\\n[SKIP] Transformer model training\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STAGE 3B: TRANSFORMER MODEL TRAINING\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    from models.Transformer.training.train_bpe_transformer import main as train_transformer_main\n",
    "    \n",
    "    try:\n",
    "        print(f\"\\nParameters:\")\n",
    "        print(f\"  Epochs: {TRANSFORMER_EPOCHS}\")\n",
    "        print(f\"  Batch size: {TRANSFORMER_BATCH_SIZE}\")\n",
    "        print(f\"  Sequence length: {TRANSFORMER_SEQ_LENGTH}\")\n",
    "        print(f\"  Learning rate: {TRANSFORMER_LEARNING_RATE}\")\n",
    "        \n",
    "        train_transformer_main(\n",
    "            batch_size=TRANSFORMER_BATCH_SIZE,\n",
    "            seq_length=TRANSFORMER_SEQ_LENGTH,\n",
    "            num_epochs=TRANSFORMER_EPOCHS,\n",
    "            learning_rate=TRANSFORMER_LEARNING_RATE\n",
    "        )\n",
    "        print(f\"\\n[OK] Transformer training complete\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n[ERROR] Transformer training failed: {e}\")\n",
    "        raise\n",
    "\n",
    "# Run Transformer training stage\n",
    "stage_train_transformer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38087ae",
   "metadata": {},
   "source": [
    "## Stage 4: Embeddings Generation\n",
    "Generate embeddings using all models and store in database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c973af72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STAGE 4: EMBEDDINGS GENERATION\n",
      "================================================================================\n",
      "\n",
      "Parameters:\n",
      "  Models: byt5, canine, bpe-lstm, bpe-transformer, bert\n",
      "  Clear tables: True\n",
      "PyTorch version: 2.6.0.dev20241112+cu121\n",
      "OS: Windows AMD64\n",
      "üöÄ CUDA device: NVIDIA GeForce RTX 3050 Laptop GPU\n",
      "\n",
      "üöÄ Running pipeline for 5 model(s): byt5, canine, bpe-lstm, bpe-transformer, bert\n",
      "Dataset: c:\\Users\\nick\\Desktop\\DTU Courses\\02456 Deep Learning\\Deep-Learning-Transformers\\data_filtered\\corpus_filtered.jsonl\n",
      "Database: postgresql+psycopg://nick:secret@localhost:5433/vectordb\n",
      "Clear existing tables: True\n",
      "\n",
      "\n",
      "============================================================\n",
      "Processing: ByT5\n",
      "============================================================\n",
      "--- Clearing existing table: byt5_small ---\n",
      "--- Clearing existing table: byt5_small ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nick\\Desktop\\DTU Courses\\02456 Deep Learning\\Deep-Learning-Transformers\\.venv\\Lib\\site-packages\\sqlmodel\\main.py:644: SAWarning: This declarative base already contains a class with the same class name and module name as pipeline.run_all_embeddings.DynamicTable, and will be replaced in the string-lookup table.\n",
      "  DeclarativeMeta.__init__(cls, classname, bases, dict_, **kw)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading ByT5 Model: google/byt5-small ---\n",
      "Using device: cuda\n",
      "--- Dataset: c:\\Users\\nick\\Desktop\\DTU Courses\\02456 Deep Learning\\Deep-Learning-Transformers\\data_filtered\\corpus_filtered.jsonl ---\n",
      "--- Table: byt5_small ---\n",
      "--- Batch size: 64 ---\n",
      "--- Dataset: c:\\Users\\nick\\Desktop\\DTU Courses\\02456 Deep Learning\\Deep-Learning-Transformers\\data_filtered\\corpus_filtered.jsonl ---\n",
      "--- Table: byt5_small ---\n",
      "--- Batch size: 64 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding with ByT5: 116it [01:01,  1.88it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ByT5 completed successfully!\n",
      "--- Cleaning up memory for ByT5 ---\n",
      "    GPU fully cleared and synchronized\n",
      "    Memory cleanup complete\n",
      "\n",
      "\n",
      "============================================================\n",
      "Processing: Canine\n",
      "============================================================\n",
      "--- Clearing existing table: canine_s ---\n",
      "--- Loading CANINE Model: google/canine-s ---\n",
      "Using device: cuda\n",
      "    GPU fully cleared and synchronized\n",
      "    Memory cleanup complete\n",
      "\n",
      "\n",
      "============================================================\n",
      "Processing: Canine\n",
      "============================================================\n",
      "--- Clearing existing table: canine_s ---\n",
      "--- Loading CANINE Model: google/canine-s ---\n",
      "Using device: cuda\n",
      "--- Dataset: c:\\Users\\nick\\Desktop\\DTU Courses\\02456 Deep Learning\\Deep-Learning-Transformers\\data_filtered\\corpus_filtered.jsonl ---\n",
      "--- Table: canine_s ---\n",
      "--- Batch size: 64 ---\n",
      "--- Dataset: c:\\Users\\nick\\Desktop\\DTU Courses\\02456 Deep Learning\\Deep-Learning-Transformers\\data_filtered\\corpus_filtered.jsonl ---\n",
      "--- Table: canine_s ---\n",
      "--- Batch size: 64 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding with Canine: 64it [00:37,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing doc: CUDA out of memory. Tried to allocate 196.00 MiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 10.01 GiB is allocated by PyTorch, and 461.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding with Canine: 65it [01:10,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing doc: CUDA out of memory. Tried to allocate 3.12 GiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 7.02 GiB is allocated by PyTorch, and 2.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding with Canine: 66it [01:45,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing doc: CUDA out of memory. Tried to allocate 3.17 GiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 7.10 GiB is allocated by PyTorch, and 2.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding with Canine: 67it [02:18,  3.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing doc: CUDA out of memory. Tried to allocate 3.21 GiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 7.18 GiB is allocated by PyTorch, and 2.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding with Canine: 68it [02:52,  5.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing doc: CUDA out of memory. Tried to allocate 3.26 GiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 7.26 GiB is allocated by PyTorch, and 2.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding with Canine: 69it [03:27,  7.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing doc: CUDA out of memory. Tried to allocate 3.31 GiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 7.34 GiB is allocated by PyTorch, and 2.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding with Canine: 70it [04:11, 10.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing doc: CUDA out of memory. Tried to allocate 4.36 GiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 4.89 GiB is allocated by PyTorch, and 1.54 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding with Canine: 71it [04:51, 14.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing doc: CUDA out of memory. Tried to allocate 4.42 GiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 4.94 GiB is allocated by PyTorch, and 1.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding with Canine: 72it [06:22, 25.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing doc: CUDA out of memory. Tried to allocate 11.21 GiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 7.17 GiB is allocated by PyTorch, and 2.34 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding with Canine: 73it [07:41, 34.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing doc: CUDA out of memory. Tried to allocate 11.36 GiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 7.24 GiB is allocated by PyTorch, and 3.17 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding with Canine: 74it [07:55, 30.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing doc: CUDA out of memory. Tried to allocate 1.58 GiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 5.14 GiB is allocated by PyTorch, and 4.51 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding with Canine: 75it [09:39, 46.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing doc: CUDA out of memory. Tried to allocate 11.68 GiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 7.39 GiB is allocated by PyTorch, and 2.33 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding with Canine: 76it [11:21, 60.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing doc: CUDA out of memory. Tried to allocate 11.83 GiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 7.46 GiB is allocated by PyTorch, and 2.34 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding with Canine: 77it [12:43, 65.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing doc: CUDA out of memory. Tried to allocate 11.99 GiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 7.54 GiB is allocated by PyTorch, and 2.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding with Canine: 78it [14:32, 77.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing doc: CUDA out of memory. Tried to allocate 12.14 GiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 7.61 GiB is allocated by PyTorch, and 2.33 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding with Canine: 79it [16:19, 85.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing doc: CUDA out of memory. Tried to allocate 12.30 GiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 7.69 GiB is allocated by PyTorch, and 2.32 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding with Canine: 80it [16:21, 61.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing doc: CUDA out of memory. Tried to allocate 1.88 GiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 5.88 GiB is allocated by PyTorch, and 4.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding with Canine: 81it [16:42, 49.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing doc: CUDA out of memory. Tried to allocate 1.90 GiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 5.93 GiB is allocated by PyTorch, and 4.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding with Canine: 82it [17:02, 41.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing doc: CUDA out of memory. Tried to allocate 1.92 GiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 5.98 GiB is allocated by PyTorch, and 4.04 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding with Canine: 83it [17:22, 35.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing doc: CUDA out of memory. Tried to allocate 1.95 GiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 6.03 GiB is allocated by PyTorch, and 3.98 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding with Canine: 84it [17:43, 31.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing doc: CUDA out of memory. Tried to allocate 1.97 GiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 6.08 GiB is allocated by PyTorch, and 3.93 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding with Canine: 85it [18:05, 28.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing doc: CUDA out of memory. Tried to allocate 1.99 GiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 6.13 GiB is allocated by PyTorch, and 3.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding with Canine: 86it [18:46, 32.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing doc: CUDA out of memory. Tried to allocate 2.02 GiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 6.18 GiB is allocated by PyTorch, and 3.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding with Canine: 87it [18:49, 23.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing doc: CUDA out of memory. Tried to allocate 2.04 GiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 6.23 GiB is allocated by PyTorch, and 3.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding with Canine: 88it [19:12, 23.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing doc: CUDA out of memory. Tried to allocate 2.06 GiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 6.28 GiB is allocated by PyTorch, and 3.73 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding with Canine: 89it [19:34, 22.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing doc: CUDA out of memory. Tried to allocate 2.09 GiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 6.33 GiB is allocated by PyTorch, and 3.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding with Canine: 90it [19:58, 23.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing doc: CUDA out of memory. Tried to allocate 2.11 GiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 6.38 GiB is allocated by PyTorch, and 3.63 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding with Canine: 91it [20:21, 23.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing doc: CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 6.44 GiB is allocated by PyTorch, and 3.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding with Canine: 92it [22:58, 63.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing doc: CUDA out of memory. Tried to allocate 552.00 MiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 8.91 GiB is allocated by PyTorch, and 1.57 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding with Canine: 93it [23:03, 45.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing doc: CUDA out of memory. Tried to allocate 2.18 GiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 6.54 GiB is allocated by PyTorch, and 3.95 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding with Canine: 94it [24:00, 49.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing doc: CUDA out of memory. Tried to allocate 564.00 MiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 9.07 GiB is allocated by PyTorch, and 1.46 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding with Canine: 94it [25:39, 16.38s/it]\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 34\u001b[39m\n\u001b[32m     31\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# Run embeddings stage\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m \u001b[43mstage_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36mstage_embeddings\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     21\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Models: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(models)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     22\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Clear tables: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEMBEDDINGS_CLEAR_EXISTING\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m     results = \u001b[43mrun_embeddings_pipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclear_existing\u001b[49m\u001b[43m=\u001b[49m\u001b[43mEMBEDDINGS_CLEAR_EXISTING\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m[OK] Embeddings generation complete\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nick\\Desktop\\DTU Courses\\02456 Deep Learning\\Deep-Learning-Transformers\\pipeline\\run_all_embeddings.py:351\u001b[39m, in \u001b[36mrun_embeddings_pipeline\u001b[39m\u001b[34m(models, clear_existing)\u001b[39m\n\u001b[32m    349\u001b[39m results = {}\n\u001b[32m    350\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m model_key \u001b[38;5;129;01min\u001b[39;00m models_to_run:\n\u001b[32m--> \u001b[39m\u001b[32m351\u001b[39m     success = \u001b[43mrun_pipeline_for_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODELS\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodel_key\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclear_existing\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclear_existing\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    352\u001b[39m     results[model_key] = success\n\u001b[32m    354\u001b[39m \u001b[38;5;66;03m# Print summary\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nick\\Desktop\\DTU Courses\\02456 Deep Learning\\Deep-Learning-Transformers\\pipeline\\run_all_embeddings.py:220\u001b[39m, in \u001b[36mrun_pipeline_for_model\u001b[39m\u001b[34m(model_config, clear_existing)\u001b[39m\n\u001b[32m    218\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(text_buffer) >= model_config[\u001b[33m'\u001b[39m\u001b[33mbatch_size\u001b[39m\u001b[33m'\u001b[39m]:\n\u001b[32m    219\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(embedder, \u001b[33m'\u001b[39m\u001b[33mgenerate_embeddings_batch\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m220\u001b[39m         vectors = \u001b[43membedder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_embeddings_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_buffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    222\u001b[39m         vectors = [embedder.generate_embedding(text) \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m text_buffer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nick\\Desktop\\DTU Courses\\02456 Deep Learning\\Deep-Learning-Transformers\\tokenization\\our_tokenizers\\Canine\\Canine_embedding.py:95\u001b[39m, in \u001b[36mCanineEmbedder.generate_embeddings_batch\u001b[39m\u001b[34m(self, texts)\u001b[39m\n\u001b[32m     79\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     80\u001b[39m \u001b[33;03mGenerate embeddings for multiple texts simultaneously (GPU batching).\u001b[39;00m\n\u001b[32m     81\u001b[39m \u001b[33;03m\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     86\u001b[39m \u001b[33;03m    List of embedding vectors (each is 768 floats)\u001b[39;00m\n\u001b[32m     87\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     88\u001b[39m \u001b[38;5;66;03m# A. Tokenize all texts at once (automatic padding to longest in batch)\u001b[39;00m\n\u001b[32m     89\u001b[39m inputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     90\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     91\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2048\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpt\u001b[39;49m\u001b[33;43m'\u001b[39;49m\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[38;5;66;03m# B. Batch Inference\u001b[39;00m\n\u001b[32m     98\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nick\\Desktop\\DTU Courses\\02456 Deep Learning\\Deep-Learning-Transformers\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:839\u001b[39m, in \u001b[36mBatchEncoding.to\u001b[39m\u001b[34m(self, device, non_blocking)\u001b[39m\n\u001b[32m    834\u001b[39m \u001b[38;5;66;03m# This check catches things like APEX blindly calling \"to\" on all inputs to a module\u001b[39;00m\n\u001b[32m    835\u001b[39m \u001b[38;5;66;03m# Otherwise it passes the casts down and casts the LongTensor containing the token idxs\u001b[39;00m\n\u001b[32m    836\u001b[39m \u001b[38;5;66;03m# into a HalfTensor\u001b[39;00m\n\u001b[32m    837\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m is_torch_device(device) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device, \u001b[38;5;28mint\u001b[39m):\n\u001b[32m    838\u001b[39m     \u001b[38;5;28mself\u001b[39m.data = {\n\u001b[32m--> \u001b[39m\u001b[32m839\u001b[39m         k: \u001b[43mv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(v, \u001b[33m\"\u001b[39m\u001b[33mto\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(v.to) \u001b[38;5;28;01melse\u001b[39;00m v\n\u001b[32m    840\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.data.items()\n\u001b[32m    841\u001b[39m     }\n\u001b[32m    842\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    843\u001b[39m     logger.warning(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAttempting to cast a BatchEncoding to type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(device)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. This is not supported.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "def stage_embeddings():\n",
    "    \"\"\"Generate embeddings using all models and store in database\"\"\"\n",
    "    if SKIP_EMBEDDINGS:\n",
    "        print(\"\\n[SKIP] Embeddings generation\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STAGE 4: EMBEDDINGS GENERATION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    from pipeline.run_all_embeddings import run_embeddings_pipeline\n",
    "    \n",
    "    try:\n",
    "        # Prepare models to run\n",
    "        if EMBEDDINGS_MODELS is None:\n",
    "            models = ['byt5', 'canine', 'bpe-lstm', 'bpe-transformer', 'bert']\n",
    "        else:\n",
    "            models = EMBEDDINGS_MODELS\n",
    "        \n",
    "        print(f\"\\nParameters:\")\n",
    "        print(f\"  Models: {', '.join(models)}\")\n",
    "        print(f\"  Clear tables: {EMBEDDINGS_CLEAR_EXISTING}\")\n",
    "        \n",
    "        results = run_embeddings_pipeline(\n",
    "            models=models,\n",
    "            clear_existing=EMBEDDINGS_CLEAR_EXISTING\n",
    "        )\n",
    "        print(f\"\\n[OK] Embeddings generation complete\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n[ERROR] Embeddings generation failed: {e}\")\n",
    "        raise\n",
    "\n",
    "# Run embeddings stage\n",
    "stage_embeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c8bb8e",
   "metadata": {},
   "source": [
    "## Stage 5: Evaluation\n",
    "Evaluate all embedding models on retrieval task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd600cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stage_evaluation():\n",
    "    \"\"\"Evaluate all embedding models on retrieval task\"\"\"\n",
    "    if SKIP_EVALUATION:\n",
    "        print(\"\\n[SKIP] Evaluation\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STAGE 5: EVALUATION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    from tokenization.evaluation.evaluation import main as evaluation_main\n",
    "    \n",
    "    try:\n",
    "        evaluation_main()\n",
    "        print(f\"\\n[OK] Evaluation complete\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n[ERROR] Evaluation failed: {e}\")\n",
    "        raise\n",
    "\n",
    "# Run evaluation stage\n",
    "stage_evaluation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1908b2",
   "metadata": {},
   "source": [
    "## Pipeline Summary\n",
    "Display the final status and summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae8124b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ PIPELINE EXECUTION COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nConfiguration Summary:\")\n",
    "print(f\"  SKIP_PREPROCESS: {SKIP_PREPROCESS}\")\n",
    "print(f\"  SKIP_TOKENIZER: {SKIP_TOKENIZER}\")\n",
    "print(f\"  SKIP_LSTM: {SKIP_LSTM}\")\n",
    "print(f\"  SKIP_TRANSFORMER: {SKIP_TRANSFORMER}\")\n",
    "print(f\"  SKIP_EMBEDDINGS: {SKIP_EMBEDDINGS}\")\n",
    "print(f\"  SKIP_EVALUATION: {SKIP_EVALUATION}\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning-transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
