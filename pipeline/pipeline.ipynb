{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4c5d3bd",
   "metadata": {},
   "source": [
    "# PIPELINE v0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ccdcd2",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20f29cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import torch\n",
    "import os\n",
    "from typing import List, Type\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "# Database Imports\n",
    "from sqlmodel import SQLModel, Field, Session, create_engine, select\n",
    "from sqlalchemy import Column, text\n",
    "from pgvector.sqlalchemy import Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf577a8",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f7a52b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load .env file if present (this lets docker write a .env we can use locally)\n",
    "load_dotenv()\n",
    "\n",
    "# Database Connection (reads from environment; default points to local Docker DB)\n",
    "DATABASE_URL = os.environ.get(\n",
    "    \"DATABASE_URL\",\n",
    "    \"postgresql+psycopg://nick:secret@localhost:5433/vectordb\",\n",
    ")\n",
    "\n",
    "# Dataset Path (can be overridden via env)\n",
    "DATASET_PATH = os.environ.get(\n",
    "    \"DATASET_PATH\",\n",
    "    \"../data_filtered/corpus_filtered.jsonl\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ac9c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nick\\Desktop\\DTU Courses\\02456 Deep Learning\\Deep-Learning-Transformers\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Setup - Configure which model to use\n",
    "# Experiment A: BPE not ready yet\n",
    "# CURRENT_MODEL_ID = 'BPE'\n",
    "# CURRENT_TABLE_NAME = 'BPE'\n",
    "# VECTOR_DIMENSION = cuck\n",
    "# CURRENT_EMBEDDER = bpe\n",
    "\n",
    "# Experiment B: ByT5 (This one works)\n",
    "CURRENT_MODEL_ID = 'google/byt5-small'\n",
    "CURRENT_TABLE_NAME = 'byt5_small'\n",
    "VECTOR_DIMENSION = 1472\n",
    "\n",
    "# Experiment C: Canine (This one should work have not tested yet delete this if you run it)\n",
    "# CURRENT_MODEL_ID = 'google/canine-s'\n",
    "# CURRENT_TABLE_NAME = 'canine_s'\n",
    "# VECTOR_DIMENSION = 768\n",
    "\n",
    "# Experiment D: SentencePiece (Maybe coming soon who knows)\n",
    "# CURRENT_MODEL_ID = 'SentencePiece thing'\n",
    "# CURRENT_TABLE_NAME = 'sentencepiece'\n",
    "# VECTOR_DIMENSION = idk yet\n",
    "\n",
    "# Import embedders - simple relative import\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add parent directory to path (repo root from pipeline folder)\n",
    "repo_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if repo_root not in sys.path:\n",
    "    sys.path.insert(0, repo_root)\n",
    "\n",
    "from tokenization.our_tokenizers.ByT5.ByT5_embedding import ByT5Embedder\n",
    "from tokenization.our_tokenizers.Canine.Canine_embedding import CanineEmbedder\n",
    "\n",
    "# Set embedder based on experiment\n",
    "CURRENT_EMBEDDER = ByT5Embedder  # Change this to CanineEmbedder for Experiment C\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2adb7f",
   "metadata": {},
   "source": [
    "## Dynamic Database Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "102d2c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table_class(table_name: str, dim: int) -> Type[SQLModel]:\n",
    "    \"\"\"\n",
    "    Dynamically creates a SQLModel class.\n",
    "    This allows us to save data to different tables (e.g., 'bert_v1', 'bert_v2')\n",
    "    without rewriting the class code manually.\n",
    "    \"\"\"\n",
    "    # We define the class attributes dynamically\n",
    "    class DynamicTable(SQLModel, table=True):\n",
    "        __tablename__ = table_name\n",
    "        __table_args__ = {'extend_existing': True} # Allows overwriting if class exists in memory\n",
    "\n",
    "        # Mapping CSV '_id' to primary key\n",
    "        id: str = Field(primary_key=True) \n",
    "        title: str\n",
    "        text: str\n",
    "        \n",
    "        # The Vector column\n",
    "        embedding: List[float] = Field(sa_column=Column(Vector(dim)))\n",
    "\n",
    "    return DynamicTable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afb985d",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e44e61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline():\n",
    "    # A. Setup Database\n",
    "    engine = create_engine(DATABASE_URL)\n",
    "    \n",
    "    # Ensure pgvector extension exists\n",
    "    with engine.connect() as conn:\n",
    "        conn.execute(text(\"CREATE EXTENSION IF NOT EXISTS vector\"))\n",
    "        conn.commit()\n",
    "\n",
    "    # B. Define the Table Model based on configuration\n",
    "    TableClass = create_table_class(CURRENT_TABLE_NAME, VECTOR_DIMENSION)\n",
    "    SQLModel.metadata.create_all(engine)\n",
    "\n",
    "    # C. Initialize ML Model\n",
    "    embedder = CURRENT_EMBEDDER(CURRENT_MODEL_ID)\n",
    "\n",
    "    # D. Process JSONL and Insert\n",
    "    if not os.path.exists(DATASET_PATH):\n",
    "        print(f\"Error: Dataset not found at {DATASET_PATH}\")\n",
    "        return\n",
    "\n",
    "    print(f\"--- Processing JSONL: {DATASET_PATH} ---\")\n",
    "    print(f\"--- Target Table: {CURRENT_TABLE_NAME} ---\")\n",
    "\n",
    "    data_buffer = []\n",
    "    BATCH_SIZE = 100 \n",
    "\n",
    "    with Session(engine) as session:\n",
    "        # Open the JSONL file\n",
    "        with open(DATASET_PATH, mode='r', encoding='utf-8') as f:\n",
    "            \n",
    "            # Iterate line by line. \n",
    "            # We wrap 'f' with tqdm to show progress (lines processed)\n",
    "            for line in tqdm(f, desc=\"Embedding Docs\"):\n",
    "                try:\n",
    "                    if not line.strip():\n",
    "                        continue # Skip empty lines\n",
    "\n",
    "                    # 1. Parse JSON\n",
    "                    row = json.loads(line)\n",
    "\n",
    "                    # 2. Extract Data\n",
    "                    # Assuming keys are: \"_id\", \"title\", \"text\" based on your previous CSV structure\n",
    "                    doc_id = row.get('_id')\n",
    "                    title = row.get('title', '')\n",
    "                    doc_text = row.get('text', '')\n",
    "\n",
    "                    # Skip if ID is missing\n",
    "                    if not doc_id:\n",
    "                        continue\n",
    "\n",
    "                    # 3. Generate Vector\n",
    "                    # Combine title and text for better semantic search context\n",
    "                    full_content = f\"{title}: {doc_text}\"\n",
    "                    vector = embedder.generate_embedding(full_content)\n",
    "\n",
    "                    # 4. Create Record\n",
    "                    record = TableClass(\n",
    "                        id=doc_id,\n",
    "                        title=title,\n",
    "                        text=doc_text,\n",
    "                        embedding=vector\n",
    "                    )\n",
    "                    data_buffer.append(record)\n",
    "\n",
    "                    # 5. Batch Commit\n",
    "                    if len(data_buffer) >= BATCH_SIZE:\n",
    "                        session.add_all(data_buffer)\n",
    "                        session.commit()\n",
    "                        data_buffer = []\n",
    "\n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"Skipping invalid JSON line\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing doc {doc_id if 'doc_id' in locals() else 'unknown'}: {e}\")\n",
    "\n",
    "            # 6. Commit remaining records after loop finishes\n",
    "            if data_buffer:\n",
    "                session.add_all(data_buffer)\n",
    "                session.commit()\n",
    "\n",
    "    print(\"\\n--- Pipeline Finished Successfully ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23555964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading ByT5 Model: google/byt5-small ---\n",
      "Using device: cpu\n",
      "--- Processing JSONL: ../data_filtered/corpus_filtered.jsonl ---\n",
      "--- Target Table: byt5_small ---\n",
      "--- Processing JSONL: ../data_filtered/corpus_filtered.jsonl ---\n",
      "--- Target Table: byt5_small ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding Docs: 664it [12:23,  1.20s/it]"
     ]
    }
   ],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "# Run the full process\n",
    "run_pipeline()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning-transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
